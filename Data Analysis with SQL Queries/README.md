# Data Analysis with SQL Queries

This repository demonstrates the use of AI agents for data analysis using SQL queries. The project focuses on automating the querying and analysis of data from an SQL database to answer user questions. It demonstrates utilizing three different frameworks for building the AI agents for the analysis: CrewAI, LangGraph and Autogen and compares their performance. 

To ensure that the benchmarking process is fair the following was implemented:
- LLM and environment - the same configurations for the LLM and environment were used for all frameworks
- evaluating answers - the list of sample user questions that need to be answered by the AI agents can be answered with a single numerical value. The answers are then compared to the expected values. The percentage of correct answers is saved in the final results for the benchmark.
- prompt - the prompt used in all frameworks is the same prompt retrieved from langchain's documentation
- tools - the agents use three identical tools: list_tables_tool and get_schema_tool are taken from the from the tools in langchain_community. execute_sql_tool is a custom tool
- workflow - the workflow is kept as similar as possible

#### Requirements

You can install the required dependencies using pip by running:

pip install -r requirements.txt

#### Configuration

Before running the project, you need to configure the settings in the config.yaml file. This file contains configuration settings for logging, benchmarking, and API keys for external services (such as Groq and OpenAI). Make sure to enter the correct API keys and configuration options according to your environment. The config.yaml used as an example is available in the repository.

Additionally, the environment variables for API keys (GROQ_API_KEY and OPENAI_API_KEY) need to be set. You can either manually set these variables in an .env file or input them when prompted.

#### Running the Benchmark

To run the benchmark, to measure the performance of the three frameworks, use the following command:

python Data_Analysis_with_SQL_Queries_CrewAI.py
python Data Analysis with SQL Queries, LangGraph, Simple Agent.py

This will run the benchmark for the number of iterations specified in the config.yaml file. Results will be saved to the results/ directory, and metrics such as API latency and correctness will be logged in logs/.

#### Metrics and Results

After each iteration, performance metrics are collected and saved in the results/ directory. This project includes a built-in MetricsCollector that captures important performance metrics during the benchmark runs. The MetricsCollector class manages the collection and storage of these metrics. Once all iterations are complete, the collected metrics are saved to a JSON file. They are also visualized using plots that are stored in a .PNG file also in the results/ directory. The plots are saved as PNG files with the timestamp and iteration count.

Key Metrics Tracked

- Iteration Time: The total time taken for each benchmark iteration.
- API Calls: The number of API calls made during an iteration.
- API Latency: The latency (in seconds) for each API call.
- Retries: The number of times a task was retried due to failure.
- Percentage of correct answers: The percentage of correct answers generated by the application compared to the expected results.
